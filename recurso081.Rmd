---
title: <span style="color:#034a94"> **Causas - Efectos**</span>
output: html_document
css: style.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(paqueteMOD)
data("creditos")

```

</br></br>


## **Causas de la multicolinealidad**

</br>

Algunas de las causas más comunes de la multicolinealidad son:

</br>

1. **El método de recolección de los datos.**, muestras recogidas en un rango reducido de valores.

```{r, echo = F, fig.align = 'center', fig.height = 6, fig.width = 6, out.width = '50%'}
#data3.1 <- read_excel("data-ex-3-1.xls")[, -1]
#pairs(data3.1, lower.panel = myPanel.cor, diag.panel = myPanel.box, #upper.panel = panel.smooth, labels = names(data3.1))
```

 <!-- **Datos de tiempos de entrega** (*Fuente: Mongomery et al. (2002)*) -->
      
</br>


2. **Restricciones en el modelo o en la población.**

<!-- <center> -->
<!-- ```{r, echo=FALSE, out.width="60%", fig.align = "center"} -->
<!-- knitr::include_graphics("img/diagrama1.png") -->
<!-- ``` -->

<!-- Fuente: Datos de consumo de electricidad - Mongomery et al. (2002) -->
<!-- </center> -->

</br>


3. **Especificación del modelo.** Por ejemplo, al agregar términos polinomiales a un modelo cuando el rango de la predictora es pequeño.

</br>

4. **Un modelo sobredefinido.** Por ejemplo, en investigación médica donde para algunos pocos pacientes se toma una gran cantidad de predictoras. Es decir que posee más variables que observaciones.


</br></br></br>

## **Efectos de la multicolinealidad**

</br>

Algunos de los efectos más notorios de la multicolinealidad son:

</br>

1. **Inflación de las varianzas de los estimadores:** consiste en la inflación de los valores $c^*_{jj}$ en las varianzas de los estimadores $V(\widehat\beta_j^*) = \sigma^2 c^*_{jj}$, cuando se considera un modelo con variables escaladas de longitud unitaria, en cuyo caso se puede demostrar que:

$$
c^*_{jj} = \frac{1}{1 - R_j^2},
$$

</br>

donde $R_j^2$ es el coeficiente de determinación muestral obtenido de una regresión de $X_j$ (como respuesta) en función de las otras variables predictoras consideradas en el modelo (actuando como predictoras de la primera).

</br>

2. **$\boldsymbol{\widehat{\beta}_j}$ muy grandes en términos absolutos:** esto se manifiesta en una traza muy grande de la matriz $\left(\boldsymbol{X}'\boldsymbol{X}\right)^{-1}$, donde: 

$\text{traza}\left(\boldsymbol{X}'\boldsymbol{X}\right)^{-1} = \sum_{j = 1}^{p} \frac{1}{\lambda_j},\ \lambda_j > 0$ es el $j$-ésimo valor propio (usualmente ordenados de mayor a menor) de la matriz $\boldsymbol{X}'\boldsymbol{X}$.

</br>

Si la $\text{traza}\left(\boldsymbol{X}'\boldsymbol{X}\right)^{-1}$ es muy grande, mayor es la distancia entre el vector de parámetros estimados y el verdadero valor del vector de parámetros.

</br>

3. **Valores de los coeficientes con signo contrario a lo esperado:** esto puede ser causado por la presencia de multicolinealidad.


</br>

4. **Regresión significativa pero ninguna variable individualmente significativa:** otra de las maneras en que se puede manifestar la multicolinealidad grave es cuando el modelo de regresión ajustado es significativo (según la prueba $F$ de la tabla ANOVA) pero individualmente, ninguno de los coeficientes asociados a las variables predictoras resulta significativo (según las pruebas $T$ de significancia individual).


</br></br>
