---
title: <span style="color:#034a94"> **Indicadores de ajuste**</span>
author: "Modelos Estadísticos para la toma de decisiones"
output:
  html_document:
    code_folding: hide
    css: style.css
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(paqueteMODELOS)
data("creditos")

```

</br></br>

En la estimación del Modelo de Regresión Lineal Múltiple (MRLM), se recurre al Método de Mínimos Cuadrados Ordinarios (MCO), un proceso de optimización. A través de este método, se obtiene una solución que estima los coeficientes del modelo al minimizar la suma de las discrepancias entre los valores predichos y los valores reales de la variable dependiente. Es normal que en esta solución puede resultar en una suma de discrepancias sea muy diferente a cero, lo que hace importante evaluar la concordancia de los datos con el modelo establecido.

Los siguientes indicadores nos ayudan en este propósito:

</br>

## <span style="color:#034a94">**Indicadores de bondad de ajuste**</span>

Dentro de los indicadores más utilizado en la comparación de modelos están:

</br></br>

### <span style="color:#034a94">**$R^2$**</span> 

Coeficiente de determinación : mide que tanto se ajusta un modelo de regresión a los datos, es decir mide la proporción del total de la variación de $Y$ que es representada por la variación de las variables regresoras ($X_1$, $X_2$, $\dots$, $X_k$) . Toma valor de cero cuando la regresión es una linea horizontal y el modelo está formado solo por el intercepto $\beta_0$. Toma el valor de 1 cuando todas las variables predictoras y la variable dependientes están en el mismo plano (una linea recta en el caso del MRLS). 

</br></br>

### <span style="color:#034a94">**$R^2_{adj}$**</span>


Coeficiente de determinación ajustado, corresponde a una versión ajustada de $R^2$, por tamaño de muestra y número de variables predictoras.  Permite detectar si agregar una predictora mejora o no el modelo de regresión. Además permite la comparación de modelos con diferentes tipos de predictoras y diferentes tamaños de muestra.

</br></br>

### <span style="color:#034a94">**AIC**</span>

Criterio de información de Akaike.  es un indicador útil en la selección de modelos 

$$AIC = 2k - 2 \ln( \widehat{L})$$
Donde $\widehat{L}$, es el valor máximo que toma la función de verosimilitud para el modelo y $k$ es el número de parámetro en el modelo.  Al comparar los modelos presenta mejor ajuste el que tenga el valor $AIC$ más pequeño. 

</br></br>

### <span style="color:#034a94">**BIC**</span>

Criterio de información bayesiano o criterio de información de Schwarz, empleado en la selección de modelos, se basa en la información del valor máximo de la función de verosimilitud del modelo, incluyendo además del número de parámetros, el tamaño de la muestra $n$, para $n < 7$ 

$$BIC = 2k \ln(n) - 2 \ln( \widehat{L})$$
Al comparar modelos, se prefiere al modelo con valor BIC más bajo.


</br></br>

<div class="content-box-gray">
### <span style="color:#686868">**Nota**</span>


Este criterio al igual que ACI  utilizan solo cuando la variable dependiente $Y$ es la misma en todos los modelos comparados

</div>



<!-- </br> -->


<!-- ### **Log Lik** -->


<!-- Logaritmo maximo verosimil -->


</br></br>

### <span style="color:#034a94">**F**</span>

Estadístico F, construido a partir de la descomposición de la varianza de la variable dependiente $Y$ en la ANOVA, valida la existencia de por lo menos una variable independiente significativa. El valor de $F$ está asociado con el valor de $R^2$ del modelo, luego valores mas altos constituye un mayor ajuste del modelo a los datos




</br></br>

### <span style="color:#034a94">**RMSE**</span>

Raiz del error cuadrático medio, compara las estimaciones obtenidas contra los valores realies de la variable $Y$, permitiendo valorar el ajuste del modelo

$$RMSE = \sqrt{\dfrac{\sum_{i=1}^{n}(_i-\widehat{y}_1)^{2}}{n}}$$


</br></br>


### <span style="color:#FF7F00">**Ejemplo**</span>

```{r, echo=FALSE}

library(modelsummary)
data("arboles1")
arboles1$D1 = as.numeric(arboles1$mg=="GENOTIPO_2")
arboles1$D2 = as.numeric(arboles1$finca=="FINCA_2")
arboles1$D3 = as.numeric(arboles1$finca=="FINCA_3")
arboles1$peso = as.numeric(arboles1$peso)
arboles1$diametro = as.numeric(arboles1$diametro)
arboles1$altura = as.numeric(arboles1$altura)


modelo1 <- lm(peso ~ diametro + altura + D2 + D3, data = arboles1)
modelo2 <- lm(peso ~ diametro +  D2 + D3, data = arboles1)
modelo3 <- lm(peso ~ diametro + D3, data = arboles1)
modelo4 <- lm(peso ~ diametro, data = arboles1)

lista_modelos <- list(
"modelo 1" = modelo1,
"modelo 2" = modelo2,
"modelo 3" = modelo3,
"modelo 4" =modelo4
)
modelsummary(
lista_modelos # varios modelos
)

```


</br>

En este caso el modelo 1 presenta el mejor ajuste


</br></br></br>


## <span style="color:#034a94">**Falta de ajuste en el modelo de RLM**</span>

</br>

La falta de ajuste también puede ser evaluada y para el modelo de regresión múltiple se quiere saber si

</br>

$$E\left[ Y\vert X_1, X_2, \ldots, X_k\right] = \beta_0 + \beta_1X_1 + \beta_2X_2 + \cdots + \beta_kX_k,$$
</br>

es una superficie de respuesta apropiada, es decir, se quiere probar

$$
\begin{array}{l}
	H_0: E\left[ Y\vert X_1, X_2, \ldots, X_k\right] = \beta_0 + \beta_1X_1 + \beta_2X_2 + \cdots + \beta_kX_k\\
	H_1: E\left[ Y\vert X_1, X_2, \ldots, X_k\right] \ne \beta_0 + \beta_1X_1 + \beta_2X_2 + \cdots + \beta_kX_k
\end{array}
$$

</br>

Para llevar a cabo esta prueba, se necesitan replicaciones de la respuesta en las combinaciones de niveles de las variables predictoras. El procedimiento es el mismo que se estudió en RLS y se basa en la descompocisión de la SSE

</br>

<div class="content-box-blue">
$$\text{SSE} = \text{SSLOF} + \text{SSPE}$$
</div>

</br>


El procedimiento de prueba se resume en la siguiente tabla:

</br></br>

### <span style="color:#034a94">**ANOVA con prueba de falta de ajuste en el modelo de RLM**</span>

</br>

| Fuente de variación | Suma de cuadrados | Grados de libertad | Cuadrado medio | $F$ calculada |
|:--------------------|------------------:|:-------------------:|:-------------:|:-------------:|
| Regresión           | SSR               |  k                  | $\text{MSR} = \frac{\text{SSR}}{k}$ | $F_0 = \frac{\text{MSR}}{\text{MSE}}$ |
| Error               | $\text{SSE}$ | $n-p$ | $\text{MSE} = \frac{\text{SSE}}{n-p}$ | |
| $\color{gray}{\scriptsize \textit{Falta de ajuste}}$ | $\ \color{gray}{\scriptsize \textit{SSLOF}}$ | $\ \color{gray}{\scriptsize m - p}$ | $\color{gray}{\scriptsize \textit{MSLOF} = \frac{\textit{SSLOF}}{m-p}}$ | $\color{gray}{\scriptsize F_0 = \frac{\textit{MSLOF}}{\textit{MSPE}}}$ |
| $\color{gray}{\scriptsize \textit{Error Puro}}$ | $\ \color{gray}{\scriptsize \textit{SSPE}}$  | $\ \color{gray}{\scriptsize n - m}$ | $\ \color{gray}{\scriptsize \textit{MSPE} = \frac{\textit{SSPE}}{n - m}}$ | |
| Total               | SST | $n - 1$ | | |

Se rechaza $H_0$ a un nivel de significancia $\alpha$ si ${F_0 > f_{\alpha, m - p, n - m}}$. En tal caso se concluye que la superficie de respuesta no es apropiada.

</br></br></br>

## <span style="color:#034a94">**Medidas remediales**</span>

</br>

Las medidas remediales descritas en el caso de RLS también son aplicables en RLM. Con el fin de superar las deficiencias del modelo se pueden realizar transformaciones sobre la variable respuesta y/o sobre las variables predictoras.

</br>

Las transformaciones sobre la respuesta pueden ayudar en el caso de que los errores no resulten normales o la varianza no sea constante. Transformaciones sobre las variables predictoras resultan útiles cuando la superficie de respuesta es curvilínea.

</br>

Si las desviaciones respecto al supuesto de normalidad son severas, y ninguna transformación resulta útil y/o interpretable, existe otra alternativa, los llamados *modelos lineales generalizados* con los cuales se pueden modelar respuestas que no se distribuyen normales; sin embargo, tales modelos están más allá del alcance de este curso.

</br></br></br>

## <span style="color:#034a94">**Identificación de observaciones extremas en el modelo de RLM**</span>

</br>

Además de la validación de supuestos en los errores de un modelo de RLM, se debe chequear la presencia de observaciones extremas, tales como:

</br>

* Observaciones atípicas (outliers)

* Puntos de balanceo

* Observaciones influenciales

</br></br></br>



## <span style="color:#034a94">**Observaciones atípicas**</span>

</br>

Una observación **atípica** (o **outlier**) es aquella que está separada (en su valor de la respuesta $Y$) del resto de las observaciones y por tanto puede afectar los resultados del ajuste del modelo de regresión.

</br>

Interesa identificarlas para luego, si es posible analizar si se tratan de observaciones malas (por errores de registro o medición) que pueden ser descartadas, o si realmente son datos correctos pero extraños que no deben ser eliminados del conjunto de datos.

</br>

Para detectar observaciones atípicas se usan los *residuales escalados* definidos anteriormente. Se considera que una observación es **atípica** cuando su residual estudentizado $r_i$, es tal que: $\vert r_i\vert > 3$.

</br>

Muchos **outliers** en los datos pueden causar niveles de confianza reales menores de lo esperado.

<!-- </br> -->

<!-- La siguiente Figura ilustra el caso de **dos observaciones atípicas**.$\vspace{-0.25cm}$ -->
<!-- ```{r fig35a, echo = F, fig.align = 'center', out.width = '78%'} -->
<!-- #knitr::include_graphics("figures/grafobsatipica.png") -->
<!-- ``` -->


</br></br></br>

## <span style="color:#034a94">**Puntos de balanceo**</span>

</br>


Un **punto de balanceo** es una observación en el espacio de las predictoras, alejada del resto de la muestra y que puede controlar ciertas propiedades del modelo ajustado.

</br>

Este tipo de observaciones posiblemente no afecte los coeficientes de regresión estimados pero sí las estadísticas de resumen como el $R^2$ y los errores estándar de los coeficientes estimados.

</br>

Los puntos de balanceo son detectados mediante el análisis de los elementos de la diagonal principal de la matriz $\boldsymbol{H}$, los $h_{ii}$, que proporcionan una medida estandarizada de la distancia de la $i$-ésima observación al centro del espacio definido por las predictoras.


</br>

Se tiene lo siguiente:

* La media de los $h_{ii}$ es:

$$\bar{h} = \dfrac{\sum\limits_{i = 1}^n h_{ii}}{n} = \frac{\text{traza}\left(\boldsymbol{H}\right)}{n} = \frac{p}{n},$$

con $p$ el número de variables predictoras del modelo de RLM.

</br>

* Se asume que la observación $i$ es un **punto de balanceo** si $h_{ii} > 2p/n$, pero si $2p/n > 1$ este criterio no funciona pues los $h_{ii}$ siempre son menores que 1.

</br>

* Observaciones con $h_{ii}$ grandes y residuales $r_i$ también grandes probablemente serán influenciales.

<!-- </br> -->


<!-- La próxima Figura ilustra el caso de **una observación de balanceo**.$\vspace{-0.25cm}$ -->
<!-- ```{r fig35b, echo = F, fig.align = 'center', out.width = '78%'} -->
<!-- #knitr::include_graphics("figures/grafobspalanca.png") -->
<!-- ``` -->

</br></br></br>

## <span style="color:#034a94">**Observaciones influenciales**</span>

</br>

Una observación es **influencial** si tiene un impacto notable sobre los coeficientes de regresión ajustados, esto es, una observación influencial se dice `hala` al modelo en su dirección, es decir, una observación es influencial si su exclusión del modelo causa cambios importantes en la ecuación de regresión ajustada.

</br>

Estas observaciones se caracterizan por tener un valor moderadamente inusual tanto en el espacio de las predictoras como en la respuesta.

</br>

Después de identificar las observaciones que están alejadas con respecto a los valores de $Y$ (atípicas) y/o con respecto a sus valores en $X$ (puntos de balanceo) evaluamos si éstas son influenciales.

</br>

<!-- La Figura siguiente ilustra el caso de **una observación influyente**.$\vspace{-0.25cm}$ -->
<!-- ```{r fig35c, echo = F, fig.align = 'center', out.width = '78%'} -->
<!-- #knitr::include_graphics("figures/grafobsinfluyente.png") -->
<!-- ``` -->

</br>

Para la evaluación se cuenta con las siguientes medidas:

</br>

#### <span style="color:#034a94">**Distancia de Cook**</span>

#### <span style="color:#034a94">**Diagnóstico DFFITS**</span>

#### <span style="color:#034a94">**Diagnóstico DFBETAS**</span>

</br>

A continuación se presentan los diagnósticos para detectar observaciones influenciales.

</br></br>

#### <span style="color:#034a94"> 1. **Distancia de Cook:**</span> 

Es una medida de la distancia cuadrática entre,
el estimador de $\boldsymbol\beta$ por mínimos cuadrados basado en las $n$ observaciones, y el estimador de $\boldsymbol\beta$ obtenido eliminando la $i$-ésima observación, así:

<div class="content-box-blue">
$$D_i = \frac{\left(\boldsymbol{\widehat{\beta}}_{\left(i\right)} - \boldsymbol{\widehat{\beta}}\right)'\boldsymbol{X'X}\left(\boldsymbol{\widehat{\beta}}_{\left(i\right)} - \boldsymbol{\widehat{\beta}}\right)}{p\,\text{MSE}} = \frac{r_i^2}{p}\left(\frac{h_{ii}}{1 - h_{ii}}\right),\ i = 1, \ldots, n$$
</div>

donde, $\boldsymbol{\widehat{\beta}}_{\left(i\right)}$ es el vector de parámetros estimados obtenido cuando no se considera en el ajuste del modelo a la observación $i$.

Note que si $D_i$ es alto entonces la observación $i$ tiene influencia sobre el vector de parámetros estimados $\boldsymbol{\widehat{\beta}}$.

</br></br>


<div class="content-box-gray">
### <span style="color:#686868"> **Notas**</span>

* Si $D_i = f_{0.5, p, n - p}$ entonces al eliminar el punto $i$ se movería $\boldsymbol{\widehat{\beta}_{\left(i\right)}}$ hacia la frontera de una región de confianza aproximada del 50\% para $\boldsymbol{\beta}$, basándose en el conjunto completo de datos, lo cual es un desplazamiento grande e indica que el estimador por mínimos cuadrados es sensible al $i$-ésimo punto de datos.

* Como $f_{0.5, p, n - p}\approx 1$ se dice que la observación $i$ será **influencial** si $D_i > 1$.

</div>

</br>

#### <span style="color:#034a94">2. **Diagnóstico DFFITS:**</span> 

Es el número de desviaciones estándar que el valor ajustado $\widehat{y}_i$ se mueve si la observación $i$ es omitida,

<div class="content-box-blue">
$$
\text{DFFITS}_i = \frac{\widehat{Y}_i - \widehat{Y}_{\left(i\right)}}{\sqrt{\text{MSE}_{\left(i\right)}\,h_{ii}}} = \frac{e_i}{\sqrt{MSE_{\left(i\right)}\left(1 - h_{ii}\right)}}\left(\frac{h_{ii}}{1 - h_{ii}}\right)^{1/2}
$$

</div>

donde, $\widehat{Y}_{\left(i\right)}$ es el $i$-ésimo valor ajustado obtenido cuando no se considera en el ajuste del modelo a la observación $i$ y $\text{MSE}_{\left(i\right)}$ es el cuadrado medio del error obtenido cuando no se considera en el ajuste del modelo a la observación $i$.

Una observación será **influencial** si $\vert\text{DFFITS}_i\vert > 2\sqrt{\frac{p}{n}}$.

</br>

#### <span style="color:#034a94">3. **Diagnóstico DFBETAS:**</span> 

Indica cuánto cambia el $j$-ésimo coeficiente de regresión estimado $\widehat{\beta}_j$ en unidades de desviación estándar, si se omite la $i$-ésima observación,


<div class="content-box-blue">
$$\text{DFBETAS}_{j\left(i\right)} = \frac{\widehat{\beta}_j - \widehat{\beta}_{j\left(i\right)}}{\sqrt{\text{MSE}_{\left(i\right)}\,c_{jj}}}$$
</div>

donde $c_{jj}$ es el $j$-ésimo elemento en la diagonal principal de la matriz $(\boldsymbol{X'X})^{-1}$ y $\text{MSE}_{\left(i\right)}$ es el MSE de la regresión sin la observación $i$.

Una observación será **influencial** si $\vert \text{DFBETAS}_{j\left(i\right)}\vert > 2/\sqrt{n}$.

</br>

<div class="content-box-gray">
### <span style="color:#686868"> **Nota**</span>

Tanto los $D_i$, como los DFFITS y los DFBETAS se pueden afectar tanto por un error de ajuste grande como por un gran balanceo, por eso, los puntos que sean detectados por estos criterios deben ser investigados.

</div>