---
title: <span style="color:#034a94"> **Asociaciones**</span>
author: "Modelos Estadísticos para la toma de decisiones"
output:
  html_document:
    code_folding: hide
    css: style.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA)
# devtools::install_github("dgonxalex80/paqueteMODELOS", force = TRUE) # descarga paquete
library(paqueteMODELOS)
data("creditos")

```

</br></br>

# <span style="color:#034a94">**Correlación**</span>

La correlación entre dos variables es un concepto fundamental en ciencia de datos, pues permite medir el grado de asociación que entre un par de variables que permite la identificación de patrones, tendencias y asociaciones en conjunto de datos. Permite identifica las relaciones a traves de los cambios que se producen o no sobre una variables cuando otra cambia. 

La magnitud de este indicador es numérico y dependiendo las características de las variables (tipo y escala de medición) son utilizados diversos coeficientes como son:

* **Coeficiente de Correlación de Pearson** : Mide la relación lineal entre dos variables continuas. Varía entre -1 y 1, con 0 indicando una correlación nula.

* **Coeficiente de Correlación de Spearman**: o correlación de rango de Spearman, se utiliza para medir la relación entre dos variables en escalas de intervalo, principalmente producidas por la cuantificación de un cuestionario que mide una caracteristica cualitativa a traves de la suma de puntuaciones obtenidas de las respuestas dadas. Utilizan los rangos de los valores para calcular la correlación.

* **Coeficiente de Correlación de Kendall**: es similar al coeficiente de Spearman, mide la concordancia entre las clasificaciones de dos variables, independientemente del valor numérico absoluto.

* **Coeficiente de Correlación Parcial** : Utilizado para medir la relación entre dos variables después de haber eliminado el efecto de otras variables relacionadas, lo que permite examinar la asociación única entre esas dos variables.

* **Coeficiente de Concordancia**: Usado para medir la similitud o acuerdo entre dos clasificadores o evaluadores en relación con una variable categórica.

* **Coeficiente de Asociación**: En el contexto de tablas de contingencia, como las utilizadas en análisis de datos categóricos, este coeficiente mide la asociación o dependencia entre dos variables categóricas.

* **Coeficiente de Incluido de Goodman y Kruskal** : Utilizado para medir la fuerza y dirección de la asociación entre dos variables categóricas ordinales.

</br></br>

# <span style="color:#034a94">**Exploración de posibles asociaciones**</span>

</br>

Inicialmente, puede ser útil realizar chequeo gráfico de la naturaleza y la fuerza de las asociaciones entre las variables predictoras con la variable respuesta, y aún entre predictoras.

</br>

Una matriz de **gráficas de dispersión** es la herramienta más útil para visualizar rápida y simultáneamente estas relaciones. Si las variables predictoras se asocian linealmente a la variable respuesta, los gráficos de dispersión respectivos deben presentar las nubes de puntos tendiendo a una línea recta. También se puede chequear si existen relaciones de tipo no lineal entre las distintas variables, y la presencia de observaciones atípicas.



</br>

Por otra parte, se espera que entre las predictoras no existan relaciones lineales fuertes ($\rho=0$), pues de lo contrario, habría información que podría ser redundante en el modelo, y se tendría un  **problema de multicolinealidad**, lo cuál se estudiará en mayor detalle más adelante en la asignatura. El **coeficiente de correlación** permite medir la intensidad de la relación lineal que existe entre las variables independientes.

</br>

Es útil también acompañar este análisis gráfico con la matriz de correlaciones de las variables del modelo, la cual muestra los coeficientes de correlación entre la variable respuesta con cada una de las predictoras y también todas las correlaciones entre las predictoras.


</br></br></br>


### <span style="color:#FF7F00"> **Ejemplo**</span>

</br>

En la siguiente figura se presentan algunas matrices de gráficos de dispersión para un conjunto de datos sobre tres variables y su prelación con la variable dependiente.

</br>

La base ausentismo  contenida en paqueteMOD contiene las variables 

* **id**     : identificación del empleado
* **ausen**  : número de días ausentes en el trabajo, en este caso variable dependiente 
* **taller** : si labora en el taller
* **sexo**   : sexo del trabajador
* **edad**   : edad del trabajador
* **antg**   : antigüedad en la empresa
* **sala**   : salario del empleado


El diagrama de dispersión permite visualizar la relación lineal que puede existir entre dos variables numéticas, para ello  se construye la siguiente matriz de diagramas.

</br></br>

<center>
```{r, echo=FALSE}
data=ausentismo[,c(2,5,6,7)]
plot(data, las=1)
```

Diagrama de dispersión
</center>

</br>


Este diagrama permite visualizar la fuerte relación que existe entre las variables **edad** y **antigüedad**, es decir que estas dos variables no son independientes y podrían generar problema en el momento que las integremos a un mismo modelo como se verá más adelante.

</br></br>

<center>
```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
cor(data) %>% 
round(.,3)
```
Matriz de correlaciones
</center>

</br>

La información contenida en la matriz de correlaciones permite valorar la magnitud relación lineal entre las variables y el sentido de la misma. $\widehat{\rho}_{_{\text{edad, antg}}} = 0.872$, lo que indica que a mayor edad, más antigüedad


</br></br>

<center>
```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.align='center'}
library(paqueteMODELOS)
data("ausentismo")
data=ausentismo[,c(2,5,6,7)]
library(GGally)
ggpairs(data, title=" ") 
```
</center>

</br></br>


```{r fig35, echo = F, fig.cap = '', fig.show = "hold", out.width = "25%", fig.scap='e'}
# knitr::include_graphics(c("figures/MGDS.png", "figures/MGDH.png"))

#knitr::include_graphics(c("figures/MGDB.png", "figures/MGDC.png"))
```


</br></br>

* **Matrices de diagramas de dispersión:** Simple (abajo izquierda), 

</br>

* **Diagrama de desnsidad** en la diagonal

</br>

* **Correlaciones**  parte superior derecha de la matriz, acompañadas de su significancia $(Ho: \rho = 0 \text{ vs } Ha: \rho \neq 0)$

</br>

Se puede observar que la mayor relación de la variable $y$ (ausen) con alguna de las variables independientes es con antg (antigüedad en la empresa) y tiene signo negativo lo cual indica que los empleados mas antiguo tienen menos ausentismo, de igual manera que las variables edad y salario.

</br>

También es importante notar que entre las variables independientes existen fuertes correlaciones, lo cual puede generar problemas en la estimación del modelo y su inferencia. Este tema (multicolinealidad) será abordado más adelante.

</br></br>

<div class="content-box-gray">

### <span style="color:#686868"> **Nota**</span>

Las variables denominadas independientes  deben de ser independientes unas de otras, por tanto sus correlaciones deberán ser no significativas al realizar pruebas de hipótesis ( $Ho: \rho = 0$).

En caso de haber una relación perfecta entre dos variables regresoras ($X_{1} = aX_{2}$), el método MCO no tendrá solución (problema de la multicolinealidad perfecta).


</div>

