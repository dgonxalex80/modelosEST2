---
title: <span style="color:#034a94"> **Modelo de Regresión Lineal Múltiple**</span>
author: "Modelos Estadísticos para la toma de decisiones"
output:
  html_document:
    code_folding: hide
    css: style.css
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(paqueteMODELOS)
data("creditos")

```
</br></br>

## <span style="color:#034a94"> **Prueba de la significancia**</span> 

</br>

Considere el caso en que se desea probar simultáneamente la significancia de 2 ó más coeficientes de la regresión, reunidos en el subconjunto $A$, en la presencia de otros coeficientes de regresión, reunidos en el subconjunto $B$. Para lo cual, se debe separar la *importancia* de los coeficientes de regresión del subconjunto $A$ dado que los coeficientes de regresión en el subconjunto $B$ están presentes en el modelo.

</br>

Una forma de medir la *importancia* de un subconjunto de coeficientes en un modelo de RLM es a través de las denominadas **sumas de cuadrados extra**.

</br>

Una **suma de cuadrados extra** (**SSextra**) mide la reducción marginal en la **SSE** (o el incremento marginal en la SSR) producida por uno o varios coeficientes de regresión, dado que los otros coeficientes de regresión están presentes en el modelo.

</br>

Una notación para las **SSextra** en un modelo de **RLM** debe definir:

* El subconjunto $A$ de coeficientes de regresión del que se quiere obtener la SSextra.

* El subconjunto $B$ de coeficientes de regresión que acompañan al subconjunto $A$ en el modelo.

</br>

Se debe cumplir que $A \cup B$ debe estar incluido en el conjunto de todos los coeficientes de regresión del modelo, y $A \cap B = \phi$.

Así, una suma de cuadrados extra para el subconjunto $A$ dado un subconjunto $B$ se denota y calcula como:

</br>

$$\text{SSR}\left(A\vert B\right) = \text{SSR}\left(A \cup B\right) - \text{SSR}\left(B\right) = \text{SSE}\left(B\right) - \text{SSE}\left(A \cup B\right)$$

</br></br>




### <span style="color:#FF7F00"> **Ejemplo**</span>

Suponga un modelo de regresión múltiple de una respuesta $Y$ en función de tres variables predictoras $X_1, X_2, X_3$, esto es,

$$Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_3 + \varepsilon$$


</br>

Veamos algunas de las posibles sumas de cuadrados extras:

</br>
$$
\begin{align*}
\text{SSR}\left(\beta_1\vert \beta_0, \beta_2, \beta_3\right) & =  & \text{SSR}\left(\beta_0, \beta_1, \beta_2, \beta_3\right) - \text{SSR}\left(\beta_0, \beta_2, \beta_3\right)\\
         & = & \text{SSE}\left(\beta_0, \beta_2, \beta_3\right) - \text{SSE}\left(\beta_0, \beta_1, \beta_2, \beta_3\right) 
\end{align*}
$$

donde $\text{SSR}\left(\beta_1\vert \beta_0, \beta_2, \beta_3\right)$  es la suma de cuadrados extra de $\beta_1$ dado que $\beta_0$, $\beta_2$ y $\beta_3$ están presentes en el modelo de regresión.

</br>

$$
\begin{align*}
\text{SSR}\left(\beta_1, \beta_2\vert \beta_0, \beta_3\right) & = & \text{SSR}\left(\beta_0, \beta_1, \beta_2, \beta_3\right) - \text{SSR}\left(\beta_0, \beta_3\right) \\
   & = & \text{SSE}\left(\beta_0, \beta_3\right) - \text{SSE}\left(\beta_0, \beta_1, \beta_2, \beta_3\right)
\end{align*}
$$

donde  $\text{SSR}\left(\beta_1, \beta_2\vert \beta_0, \beta_3\right)$ es la suma de cuadrados extra de $\beta_1$ y $\beta_2$ dado que $\beta_0$ y $\beta_3$ están presentes en el modelo de regresión.

</br>

$$
\begin{align*}
\text{SSR}\left(\beta_1\vert \beta_0,\beta_3\right) &=& \text{SSR}\left(\beta_0, \beta_1, \beta_3\right) - \text{SSR}\left(\beta_0, \beta_3\right)\\
&=& \text{SSE}\left(\beta_0, \beta_3\right) - \text{SSE}\left(\beta_0, \beta_1, \beta_3\right)
\end{align*}
$$

donde $\text{SSR}\left(\beta_1\vert \beta_0,\beta_3\right)$  es la suma de cuadrados extras de $\beta_1$ dado que $\beta_0$ y $\beta_3$ están presentes en el modelo de regresión.

<!-- (**Tarea:** defina la suma de cuadrados extra $\text{SSR}\left(\beta_2\vert \beta_0, \beta_1\right)$) -->

</div>

</br></br>

Volviendo al objetivo inicial donde se desea probar simultáneamente la significancia de 2 o más coeficientes de la regresión, por ejemplo, en el modelo $Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_3 + \beta_4X_4 + \beta_5X_5 + \varepsilon$, se desea probar si el subconjunto de coeficientes de regresión $\beta_1, \beta_2$ y $\beta_5$ es significativo en el modelo, esto es, se desea probar que:

$$
\begin{array}{l}
H_0:\ \beta_1 = \beta_2 = \beta_5 = 0\\
H_1:\ \text{Algún }\beta_j\neq 0,\quad j = 1, 2, 5.
\end{array}
$$

</br>

Para este tipo de pruebas se requiere calcular las sumas de cuadrados extra asociada al subconjunto de los coeficientes de regresión $A = \left\{\beta_1, \beta_2, \beta_5\right\}$ dado el subconjunto de coeficientes restante $B = \left\{\beta_0, \beta_3, \beta_4\right\}$.



Esto es,


$$
\begin{aligned}
\text{SSR}&\left(\beta_1, \beta_2, \beta_5 \vert \beta_0, \beta_3, \beta_4\right) =\\
&\quad\quad\quad\quad= \text{SSR}\left(\beta_0, \beta_1, \beta_2, \beta_3, \beta_4, \beta_5\right) - \text{SSR}\left(\beta_0, \beta_3, \beta_4\right)\\
&\quad\quad\quad\quad= \text{SSE}\left(\beta_0, \beta_3, \beta_4\right) - \text{SSE}\left(\beta_0, \beta_1, \beta_2, \beta_3, \beta_4, \beta_5\right)
\end{aligned}
$$

</br></br>


### <span style="color:#FF7F00"> **Ejemplo**</span>

</br>

En este cálculo se pueden definir dos modelos:

* **Un modelo completo:** que incluye todos los coeficientes de regresión que se consideran inicialmente en el modelo (el subconjunto $A \cup B$). Para el ejemplo es:
$$Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_3 + \beta_4X_4 + \beta_5X_5 + \varepsilon.$$

</br></br>

* **Un modelo nulo o reducido:** que se obtiene al aplicar lo establecido en $H_0$ al modelo completo, es decir, eliminar los coeficientes de regresión en $A$ (quedando los coeficientes de regresión en $B$). Para el ejemplo es:

$$Y = \beta_0 + \beta_3X_3 + \beta_4X_4 + \varepsilon.$$


</br>

Al igual que en las sumas de cuadrados vistas en la tabla **ANOVA**, las sumas de cuadrados extra tienen asociados unos grados de libertad, que en este caso se obtienen como el tamaño del subconjunto $A$ que se está probando, o equivalentemente como la diferencia en grados de libertad de la SSR (o SSE) de los dos modelos definidos anteriormente.

</br></br>

$$
\begin{aligned}
\text{g.l}_{\text{SSR}\left(\beta_1, \beta_2, \beta_5 \vert \beta_0, \beta_3, \beta_4\right)}  &=& \text{g.l}_{\text{SSR}\left(\beta_0, \beta_1, \beta_2, \beta_3, \beta_4, \beta_5\right)} - \text{g.l}_{\text{SSR}\left(\beta_0, \beta_3, \beta_4\right)}\\ 
&=& 5 - 2 \\
&=&  \text{g.l}_{\text{SSE}\left(\beta_0, \beta_3, \beta_4\right)} - \text{g.l}_{\text{SSE}\left(\beta_0, \beta_1, \beta_2, \beta_3, \beta_4, \beta_5\right)}\\
&=& (n - 3) - (n - 6) \\
&=& 3
\end{aligned}
$$


También se define el **cuadrado medio extra** (MSextra) como la razón entre la **suma de cuadrados extra** y sus respectivos grados de libertad. Para el ejemplo


$$\text{MSR}\left(\beta_1, \beta_2, \beta_5 \vert \beta_0, \beta_3, \beta_4\right) = \frac{\text{SSR}\left(\beta_1, \beta_2, \beta_5 \vert \beta_0, \beta_3, \beta_4\right)}{3}$$

</br>

Finalmente, el **estadístico de prueba** es igual a la razón del cuadrado medio extra sobre el cuadrado medio del error del modelo completo. Para el ejemplo, sería

</br>

$$
\begin{aligned}
F_0 &= \frac{\text{MSR}\left(\beta_1, \beta_2, \beta_5 \vert \beta_0, \beta_3, \beta_4\right)}{\text{MSE}\left(\beta_0, \beta_1, \beta_2, \beta_3, \beta_4, \beta_5\right)}\\[0.5cm]
&= \frac{\text{SSR}\left(\beta_1, \beta_2, \beta_5 \vert \beta_0, \beta_3, \beta_4\right)/3}{\text{MSE} }
\end{aligned}
$$


siempre en el denominador estará el MSE del modelo completo.

</br></br>

A un nivel de significancia $\alpha$, el criterio de rechazo es $F_0 > f_{\alpha, 3, n - 6}$.

</br></br>


### <span style="color:#FF7F00"> **Ejemplo**</span>

En el modelo $Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_3 + \beta_4X_4 + \beta_5X_5 + \varepsilon$, para probar la hipótesis:
$$
\begin{array}{l}
H_0: \beta_2 = \beta_4 = 0\\
H_1: \text{Algún }\beta_j \ne 0, \ j = 2, 4.
\end{array}
$$


</br>

se usa como estadístico de prueba a 

$$F_0 = \frac{\text{SSR}\left(\beta_2, \beta_4\vert \beta_0, \beta_1, \beta_3, \beta_5\right)/2}{\text{MSE}} \overset{\ \text{bajo }H_0}{\sim} f_{2, n - 6}$$

y con un nivel de significancia $\alpha$ el criterio de rechazo de la hipótesis nula es $F_0 > f_{\alpha, 2, n - 6}$


</br></br>

<!-- ## **Uso de SSextra para la prueba de significancia de un coeficiente individual** -->

<H2> 
<span style="color:#034a94">**Uso de SSextra para la prueba de significancia de un coeficiente individual**</span> 
</H2>
</br>

En un modelo de RLM con $k$ predictoras, esta prueba establece que

$$\begin{aligned} &H_0: \beta_j = 0\\ &H_1: \beta_j \neq 0\end{aligned}, \quad j = 1, 2, \ldots, k,$$
donde $A = \left\{\beta_j\right\}$ y $B = \left\{\beta_0, \beta_1, \beta_2, \ldots, \beta_{j - 1}, \beta_{j + 1}, \beta_{j + 2}, \ldots, \beta_k\right\}$. Luego, usando SSextra el estadístico de prueba es:

</br></br>

$$
F_{j,0} = \frac{\text{SSR}\left(\beta_j\vert \beta_0, \beta_1, \beta_2, \ldots, \beta_{j - 1}, \beta_{j + 1}, \beta_{j + 2}, \ldots, \beta_k\right)}{\text{MSE}}
$$

</br></br>

Observe que la SSextra solo tiene un grado de libertad, de forma que es igual al MSextra, y bajo la hipótesis nula $F_{j,0} \sim f_{1, n - p}$, por lo cual, a un nivel de significancia $\alpha$, el criterio de rechazo de la hipótesis nula es: $F_{j,0} > f_{\alpha, 1, n - p}$.

</br>

La prueba anterior es equivalente a la prueba $t$ definida en una clase anterior. De hecho se puede demostrar que.

* $F_{j,0} = T_{j,0}^2$.

* Si se calculan los valores-P de los dos procedimientos de prueba, se llega a que:

$$\text{vp}_F = P\left(f_{1, n - p} > F_{j, 0}\right) \equiv P\left(\vert t_{n - p}\vert > \vert T_{j, 0}\vert\right) = \text{vp}_T$$

Por otro lado, también se puede ver la prueba de significancia de la regresión como un caso particular de una prueba basada en SSextra donde $A = \left\{\beta_1, \beta_2, \ldots, \beta_k\right\}$ y $B = \left\{\beta_0\right\}$.

</br></br></br>



<H2> 
<span style="color:#034a94">**Prueba de la hipótesis lineal general**</span>
</H2>

</br>

Suponga un modelo de RLM con $k$ variables predictoras, $Y = \beta_0 + \beta_1X_1 + \cdots + \beta_kX_k + \varepsilon$, al que llamaremos modelo completo (FM).

</br>

En este modelo se tiene una suma de cuadrados de la regresión 
$\text{SSR}(\text{FM}) = \text{SSR}\left(\beta_0, \beta_1,\ldots,\beta_k\right)$ con $k$ g.l y una suma de cuadrados del error 

</br>

$\text{SSE}(\text{FM}) = \text{SSE}\left(\beta_0, \beta_1,\ldots,\beta_k\right)$ con $n - p$ g.l.

</br>

Considere además una matriz $m\times p$ de constantes $\boldsymbol{L}$, con $r \leq m$ filas linealmente independientes. Se puede formular una prueba de hipótesis lineal general como:

</br>


$$
H_0: \boldsymbol{L\beta} = \boldsymbol{0}\ \text{ vs. }\ H_1: \boldsymbol{L\beta} \neq \boldsymbol{0},
$$
</br>

donde, $\boldsymbol{0}$ es un vector de ceros de dimensión $m\times 1$.


</br>

$\boldsymbol{L\beta} = \boldsymbol{0}$ es simplemente un sistema de ecuaciones donde se formulan $m$ hipótesis que se prueban simultáneamente.

Si al modelo completo se le aplica lo establecido en $H_0$ se llega a un modelo reducido (RM), que tiene asociado tanto una suma de cuadrados de la regresión $\text{SSR}(\text{RM})$ como una suma de cuadrados del error $\text{SSE}(\text{RM})$.

Para probar la hipótesis se debe definir una **suma de cuadrados debida a la hipótesis** (SSH) que se calcula como la diferencia entre las sumas de cuadrados de la regresión (o del error) de los modelos completo y reducido. Esto es,

</br></br>


<div class="content-box-blue">

$$\text{SSH} = \text{SSE}(\text{RM}) - \text{SSE}(\text{FM}) = \text{SSR}(\text{FM}) - \text{SSR}(\text{RM})$$
donde :

* $\text{SSE}(\text{RM})$ : suma de cuadrado de los errores del modelo restringido
* $\text{SSE}(\text{FM})$ ; suma de cuadrado de los errores del modelo completo

</div>

</br>

que tiene tantos grados de libertad como el número $r$ de filas linealmente independientes en $\boldsymbol{L}$. O equivalentemente:

</br>

$$r = \text{g.l}_{\text{SSE}(\text{RM})} - \text{g.l}_{\text{SSE}(\text{FM})} = \text{g.l}_{\text{SSR}(\text{FM})} - \text{g.l}_{\text{SSR}(\text{RM})}$$

</br>

Luego, se define el **cuadrado medio debido a la hipótesis** (MSH) como:

</br>
$$\text{MSH} = \frac{\text{SSH}}{r}.$$

</br>

Finalmente, se define como estadístico de prueba a la razón entre el cuadrado medio de la hipótesis y el cuadrado medio del error del modelo completo:

</br>

$$
F_0 = \frac{\text{MSH}}{\text{MSE}\left(\beta_0, \beta_1, \ldots, \beta_4\right)} = \frac{\text{SSH}/r}{\text{MSE}}
$$

</br>

Se puede demostrar que bajo $H_0$ el estadístico $F_0\sim f_{r, n - p}$. Lo cual permite a un nivel de significancia $\alpha$, rechazar $H_0$ si $F_0 > f_{\alpha, r, n - p}$.

Veamos, ejemplos de cómo trabaja este procedimiento de prueba.

</br></br>


<H3>
<span style="color:#FF7F00"> **Ejemplo**</span>
</H3>

Suponga un modelo de RLM con $k = 4$ variables predictoras, entonces se puede formular la siguiente prueba de hipótesis:
$$H_0: \beta_1 = \beta _2,\ \beta_3 = \beta_4$$

$$H_1: \beta_1 \ne \beta_2\ \text{ ó }\ \beta_3 \ne \beta_4$$



</br>

Podemos reescribir la hipótesis nula de la siguiente manera:
$$
H_0: \beta_1 - \beta_2 = 0,\ \beta_3 - \beta_4 = 0,
$$

de manera que la hipótesis nula contiene $m = 2$ ecuaciones y se puede escribir como:
$$
H_0: \left\{\begin{array}{c}
\beta_1 - \beta_2 = 0\\
\beta_3 - \beta_4 = 0
\end{array}\right.
$$

que en forma matricial se puede expresar como:

$$
H_0: \begin{array}{ccccclll}
\left[\begin{array}{cccccc}
0 & 1 & -1 & 0 & 0\\
0 & 0 & 0 & 1 & -1
\end{array}\right] & \left[\begin{array}{c}
\beta_0\\
\beta_1\\
\beta_2\\
\beta_3\\
\beta_4
\end{array}\right] & = & \left[\begin{array}{c}
0\\
0
\end{array}\right]
\end{array}
$$

por tanto, se tiene una prueba de hipótesis lineal general, con:

$$
\boldsymbol{L} = \left[\begin{array}{cccccc}
0 & 1 & -1 & 0 & 0\\
0 & 0 & 0 & 1 & -1
\end{array}\right],
$$

que tiene $r = 2$ filas linealmente independientes (observe que una fila no puede escribirse como un múltiplo escalar de la otra).

El modelo reducido en este caso es 
$$\begin{aligned}\text{RM: }\ Y &= \beta_0 + \beta_1\left(X_1 + X_2\right) + \beta_3\left(X_3 + X_4\right) + \varepsilon\\ &= \beta_0 + \beta_1X_{1, 2} + \beta_3X_{3, 4} + \varepsilon\end{aligned},$$

donde $X_{1, 2} = X_1 + X_2$, y $X_{3, 4} = X_3 + X_4$.

</br>

En este modelo se tiene una suma de cuadrados del error $\text{SSE}(\text{RM}) = \text{SSE}\left(\beta_0, \beta_1, \beta_3\right)$ con $n - 3$ grados de libertad.

Luego, la SSH se calcula como:
$$\text{SSH} = \text{SSE}(\text{RM}) - \text{SSE}(\text{FM}),$$
que tiene 2 grados de libertad, de manera que el cuadrado medio debido a la hipótesis es:
$$\text{MSH} = \frac{\text{SSH}}{2}.$$



Finalmente, se define como estadístico de prueba a:

$$
F_0 = \dfrac{\text{SSH}/2}{\text{MSE}}
$$

</br></br>

<div class="content-box-gray">
<H3>
<span style="color:#686868"> **Nota**</span>
</H3>
 

Observe que en el denominador se encuentra el cuadrado medio del modelo completo que tiene $n - 5$ grados de libertad.

Bajo $H_0$ y los supuestos sobre los errores, $F_0\sim f_{2, n - 5}$. Se rechaza para valores grandes de este estadístico, esto es, si $VP = P\left(f_{2, n - 5} > F_0\right)$ es pequeño. O bien, si $F_0 > f_{\alpha, 2, n - 5}$, el valor crítico a un nivel de significancia $\alpha$.
</div>

</br></br>


<H3>
<span style="color:#FF7F00"> **Ejemplo**</span>
</H3>

Bajo el mismo modelo de RLM con $k=4$ considere la siguiente prueba:

$$
H_0:\ \beta_1 = \beta_2 = 0, \ \beta_3 = \beta_4 \ \ \text{ vs. }\ H_1:\ \beta_1 \ne 0 \text{ \ ó \ } \beta_2 \ne 0 \text{ \ ó \ } \beta_3 \ne \beta_4
$$

</br>

Como en el ejemplo anterior, también se puede reescribir la hipótesis nula en términos de ecuaciones igualadas a cero:

$$
H_0:\ \beta_1 = 0, \ \beta_2 = 0, \ \beta_3 - \beta_4 = 0
$$

</br>

Luego, en $H_0$ se tiene un sistema de $m=3$ ecuaciones que se puede expresar como:

</br>

$$
H_0: \begin{array}{ccccclll}
\left[\begin{array}{cccccc}
0 & 1 & 0 & 0 & 0\\
0 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & 1 & -1
\end{array}\right] & \left[\begin{array}{c}
\beta_0\\
\beta_1\\
\beta_2\\
\beta_3\\
\beta_4
\end{array}\right] & = & \left[\begin{array}{c}
0\\
0\\
0
\end{array}\right]
\end{array}
$$

</br></br>

por tanto, se tiene una prueba de hipótesis lineal general, con:

$$
\boldsymbol{L} = \left[\begin{array}{cccccc}
0 & 1 & 0 & 0 & 0\\
0 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & 1 & -1
\end{array}\right],
$$

</br></br>

que tiene $r = 3$ filas linealmente independientes (compruebe que ninguna de las filas se puede escribir como combinación lineal de las otras dos filas).

</br>

Entonces, el modelo nulo es:

</br>

$$\begin{aligned} \text{RM: }\ Y &= \beta_0 + \beta_3\left(X_3 + X_4\right) + \varepsilon\\ &= \beta_0 + \beta_3X_{3, 4} + \varepsilon\end{aligned},$$

</br>

donde $X_{3, 4} = X_3 + X_4$.

</br></br>

El estadístico de prueba es,
$$
F_0 = \dfrac{\text{SSH}/3}{\text{MSE}}.
$$

</br>

Bajo $H_0$ y los supuestos sobre los errores, $F_0\sim f_{3, n - 5}$. Se rechaza para valores grandes de este estadístico, esto es, si $\text{VP} = P\left(f_{3, n - 5} > F_0\right) < \alpha$, donde $\alpha$ es el nivel de significancia de la prueba. O bien, si $F_0 > f_{\alpha, 3, n - 5}$.

</br></br>


### <span style="color:#FF7F00"> **Ejemplo**</span>

</br>

Considere ahora la prueba de significancia del modelo de RLM con $k=4$ variables predictoras:

$$
H_0:\ \beta_1 = \beta_2 = \beta_3 = \beta_4 = 0 \quad\text{ vs. }\quad H_1:\ \text{Algún}\ \beta_j \ne 0,\ j = 1, 2, 3, 4.
$$


</br></br>

<div class="content-box-gray">
### <span style="color:#686868"> **Nota**</span>

</br>

$H_0$ se puede reescribir como:

$$
H_0:\ \beta_1 = 0,\ \beta_2 = 0,\ \beta_3 = 0,\ \beta_4 = 0.
$$
</div>

</br></br>

En este caso también se puede reformular la hipótesis nula en la forma de una hipótesis lineal general, considerando las $m = r = 4$ ecuaciones linealmente independientes como sigue:

</br>

$$
H_0: \begin{array}{ccccclll}
\left[\begin{array}{cccccc}
0 & 1 & 0 & 0 & 0\\
0 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & 1 & 0\\
0 & 0 & 0 & 0 & 1
\end{array}\right] & \left[\begin{array}{c}
\beta_0\\
\beta_1\\
\beta_2\\
\beta_3\\
\beta_4
\end{array}\right] & = & \left[\begin{array}{c}
0\\
0\\
0\\
0
\end{array}\right]
\end{array}
$$

</br>


El modelo reducido es simplemente RM: $Y = \beta_0 + \varepsilon$, donde el intercepto representa la media de la variable respuesta. Así el estimador de mínimos cuadrados del intercepto es simplemente la media muestral de $Y$, es decir, $\widehat{\beta}_0 = \bar{Y}$, por tanto, $\widehat{Y} = \bar{Y}$, y en consecuencia tiene una suma de cuadrados del error igual a la suma de cuadrados totales ($\text{SSE}\left(\beta_0\right) = \text{SST}$) con $n - 1$ grados de libertad, mientras que la suma de cuadrados de la regresión es igual a cero ($\text{SSR}\left(\beta_0\right) = 0$).

</br>

Al calcular la SSH en función de la diferencia entre las SSE de los modelos RM y FM, se obtiene:

</br>

$$\begin{aligned} \text{SSH} &= \text{SSE}\left(\beta_0\right) - \text{SSE}\left(\beta_0, \beta_1, \beta_2, \beta_3, \beta_4\right)\\ &= \text{SST} - \text{SSE}\left(\beta_0, \beta_1, \beta_2, \beta_3, \beta_4\right)\\ &= \text{SSR}\left(\beta_0, \beta_1, \beta_2, \beta_3, \beta_4\right) = \text{SSR}\end{aligned},$$
</br>

con $r = m = k = 4$ grados de libertad, cuyo MSextra coincide con el MSR del modelo completo.

</br></br>

Así, el estadístico de prueba coincide con el visto en la prueba de significancia de la regresión

</br>

$$
F_0 = \dfrac{\text{MSH}}{\text{MSE}} = \dfrac{\text{SSH}/4}{\text{MSE}} = \dfrac{\text{SSR}/4}{\text{MSE}} = \dfrac{\text{MSR}}{\text{MSE}}
$$

</br>

Por lo tanto, bajo $H_0$ y los supuestos sobre los errores se cumple que, $F_0 \sim f_{4, n - 5}$. Se rechaza para valores grandes de este estadístico, esto es, si $\text{VP} = P\left(f_{4, n - 5} > F_0\right) < \alpha$, donde $\alpha$ es el nivel de significancia de la prueba. O bien, si $F_0 > f_{\alpha, 4, n - 5}$.


</br></br>


<div class="content-box-gray">
### <span style="color:#686868"> **Retos**</span>

* Comprobar que una prueba de significancia individual o para un subconjunto de coeficientes de regresión son casos particulares de la prueba de hipótesis lineal general.)


</br></br>


* También es posible probar hipótesis lineales generales del tipo
$H_0: \boldsymbol{L\beta} = \boldsymbol{c}\ \text{ vs. }\ H_1: \boldsymbol{L\beta} \neq \boldsymbol{c}$, donde $\boldsymbol{c}$ es un vector de constantes arbitrario.
</div>



