---
title: <span style="color:#034a94"> **Propiedades de los estimadores MCO**</span>
author: "Modelos Estadísticos para la toma de decisiones"
output:
  html_document:
    code_folding: hide
    css: style.css
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# install.packages("devtools") # solo una vez
# devtools::install_github("dgonxalex80/paqueteMODELOS", force = TRUE)
library(paqueteMODELOS)
data("creditos")

```

</br></br>

Antes de empezar revisemos el concepto de estimador y sus propiedades

</br>

## <span style="color:#034a94">**Estimador estadístico**</span>

Es una función de los valores obtenidos en una muestra que se emplea para encontrar una aproximación a un parámetro población desconocido. Este proceso se puede realizarse de manera puntual o por intervalos de confianza. Los estimadores se represntan en general por  $\widehat{\theta}$ y su respectivo parámetro como $\theta$.   

<br/>

Ahora como los resultados obtenidos proceden de una muestra (la base de datos), hace que el valor del estimador encontrado sea diferente para muestras diferentes, lo cual hace que el estimador se convierta en una variable aleatoria que se puede caracterizar por una modelo de probabilidad (en el caso de los $\widehat{\beta}_{i}$por lo general t-Student), bajo el supuesto de que los errores tengan distribución normal

</br></br>

## <span style="color:#034a94">**Propidedades de los estimadores**</span>

Dentro de las princopales propiedades de los estimadores

</br>

###  <span style="color:#FF7F00"> **Insesgadez**</span>

Un estimador es insesgado cuando su valor esperado es igual al parámetro :
$$E\Big[\widehat{\beta}_{i}\Big] = \beta_{i} $$

</br>

### <span style="color:#FF7F00"> **Consistencia**</span>

Cuando un estimador es sesgado, es decir que su valor esperado no coincide con el parámetro, puede tener la propiedad de consistente, que ocurre cuando al aumentar el tamaño de la muestra el estimador se convierte en insesgado como por ejemplo:

$$s^{2} = \dfrac{\sum_{i=1}^{n} (x_{1}- \bar{x})² }{n}$$
En este caso se puede demostrar que 


$$E[s²] = \dfrac{(n-1) \sigma²}{n}$$
El termino que genera el sesgo es : $(n-1)/n$, el cual tiende a 1 cuando $n$ es grade, haciendo que el sesgo desaparesca


</br>

### <span style="color:#FF7F00"> **Eficiencia**</span>

Es el estimador que tiene la varianza mas baja al compararlo con otros estimadores, todos insesgados

</br>

### <span style="color:#FF7F00"> **Robustez**</span>

Es aquel estimador que sigue siendo insesgado a pesar de contener datos atípicos

</br></br>

# <span style="color:#034a94">**Algunas propiedades de $\widehat{\beta}$**</span>

</br>

* Los estimadores de mínimos cuadrados corresponden a los estimadores de máxima verosimilitud, bajo el modelo lineal normal.

* $\boldsymbol{\widehat\beta}$ es un estimador **insesgado** del vector de parámetros $\boldsymbol{\beta}$, es decir

<div class="content-box-blue">
$$
E\left[\boldsymbol{\widehat\beta}\right] = E\left[\left(\boldsymbol{X}'\boldsymbol{X}\right)^{-1}\boldsymbol{X}'\boldsymbol{y}\right] = \boldsymbol{\beta}
$$
</div>

</br>

En efecto, sea $\boldsymbol{A} = \left(\boldsymbol{X}'\boldsymbol{X}\right)^{-1}\boldsymbol{X}'$ una matriz $p \times n$ de valores fijos (constantes). Entonces,

</br>

$$
E\left[\boldsymbol{\widehat\beta}\right] = E\left[\boldsymbol{A}\boldsymbol{y}\right] = \boldsymbol{A}E\left[\boldsymbol{y}\right] = \boldsymbol{A\ X\beta} = \left(\boldsymbol{X}'\boldsymbol{X}\right)^{-1}\boldsymbol{X}'\ \boldsymbol{X\beta} = \boldsymbol{\beta}
$$

</br>

* La matriz de varianzas covarianzas de $\boldsymbol{\widehat\beta}$ es 


<div class="content-box-blue">
$$
V\left(\boldsymbol{\widehat\beta}\right) = \sigma^2\left(\boldsymbol{X'X}\right)^{-1}
$$
</div>

</br>

En efecto,

$$
\begin{aligned}
	V\left[\boldsymbol{\widehat\beta}\right] &= V\left[\boldsymbol{A}\boldsymbol{y}\right] = \boldsymbol{A}V\left[\boldsymbol{y}\right]\boldsymbol{A}' = \boldsymbol{A}\, \sigma^2\boldsymbol{I}_n\, \boldsymbol{A}'\\
	&= \sigma^2\left\{\left(\boldsymbol{X}'\boldsymbol{X}\right)^{-1}\boldsymbol{X}'\boldsymbol{I}_n\left[\left(\boldsymbol{X}'\boldsymbol{X}\right)^{-1}\boldsymbol{X}'\right]'\right\}\\
	&= \sigma^2\left\{\left(\boldsymbol{X}'\boldsymbol{X}\right)^{-1}\boldsymbol{X}'\,\boldsymbol{X}\left[\left(\boldsymbol{X}'\boldsymbol{X}\right)^{-1}\right]'\right\} = \sigma^2\left(\boldsymbol{X}'\boldsymbol{X}\right)^{-1},
\end{aligned}
$$
</br>


La cual tiene en su diagonal principal a las varianzas de los estimadores de los parámetros, $V\left(\widehat\beta_j\right), j = 0, 1, \ldots, k$, y por fuera de su diagonal principal a las covarianzas entre tales estimadores.

</br></br>

<div class="content-box-gray">
### <span style="color:#686868"> **Nota**</span>

$\boldsymbol{\widehat\beta} = \boldsymbol{A}\boldsymbol{y}$ implica que cada parámetro estimado es una combinación lineal de las observaciones, así que $\widehat\beta_j$ es una variable aleatoria con distribución normal (ya que los $y_i$'s son normales).
</div>

</br></br>

<div class="content-box-gray">

### <span style="color:#686868"> **Resumen**</span>

$$\boldsymbol{\widehat\beta}\sim\boldsymbol{N}\left(\boldsymbol{\beta}, \, \sigma^2\left(\boldsymbol{X}'\boldsymbol{X}\right)^{-1}\right),$$
</div>

</br>

y en el caso particular del estimador del $j$-ésimo parámetro del modelo se tiene que:

</br>

$$\widehat\beta_j\sim N\left(\beta_j, \, \sigma^2\,c_{jj}\right),\ j = 0, 1, \ldots, k.$$

</br>

Observe que en las expresiones anteriores $\sigma^2$ es desconocido, de manera que debe estimarse.

</br>

Un estimador de $\sigma^2$ surge del método de máxima verosimilitud al igual que en el modelo de Regresión Lineal Simple, el cual se define como:

</br>

$$\text{MSE} = \frac{\displaystyle\sum_{i = 1}^n \left(y_i - \widehat{y}_i\right)^2}{n - p} = \frac{\text{SSE}}{n - p}.$$
</br>


Luego, una estimación de la matriz de varianzas-covarianzas es: 

</br>

$$V\left(\boldsymbol{\widehat\beta}\right) = \text{MSE}\left(\boldsymbol{X}'\boldsymbol{X}\right)^{-1},$$

cuyos elementos en la diagonal principal corresponden a las estimaciones de las varianzas de los respectivos estimadores $\widehat\beta_j$, esto es, $$\widehat{V}\left(\widehat\beta_j\right) = \text{MSE}\,c_{jj},$$

donde $c_{jj}$, es el $j$-ésimo elemento de la diagonal de la matriz $\left(\boldsymbol{X}'\boldsymbol{X}\right)^{-1}$.


</br></br>
